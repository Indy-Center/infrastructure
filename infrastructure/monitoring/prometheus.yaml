apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: kube-prometheus-stack
  namespace: monitoring
spec:
  repo: https://prometheus-community.github.io/helm-charts
  chart: kube-prometheus-stack
  version: "69.2.0"
  targetNamespace: monitoring
  valuesContent: |-
    prometheus:
      ingress:
        enabled: false
      prometheusSpec:
        nodeSelector:
          node.kubernetes.io/role: ops
        tolerations:
          - key: "node.kubernetes.io/role"
            operator: "Equal"
            value: "ops"
            effect: "NoSchedule"

    grafana:
      ingress:
        enabled: false
      nodeSelector:
        node.kubernetes.io/role: ops
      tolerations:
        - key: "node.kubernetes.io/role"
          operator: "Equal"
          value: "ops"
          effect: "NoSchedule"
      envFromSecret: grafana-oauth
      grafana.ini:
        server:
          root_url: https://metrics.zid-internal.com
          protocol: http
        auth.github:
          enabled: true
          allow_sign_up: true
          scopes: read:org,user:email,repo
          auth_url: https://github.com/login/oauth/authorize
          token_url: https://github.com/login/oauth/access_token
          api_url: https://api.github.com/user
          allowed_organizations: indy-center
          client_id: ${GF_AUTH_GITHUB_CLIENT_ID}
          client_secret: ${GF_AUTH_GITHUB_CLIENT_SECRET}
          role_attribute_path: contains(groups[*], '@Indy-Center/devops') && 'GrafanaAdmin' || contains(groups[*], '@Indy-Center/developers') && 'Editor' || 'None'

    additionalPrometheusRulesMap:
      alert-rules:
        groups:
          - name: NodeAlerts
            rules:
              - alert: NodeNotReady
                expr: kube_node_status_condition{condition="Ready",status="true"} == 0
                for: 5m
                labels:
                  severity: critical
                annotations:
                  title: 'üî• Node {{ $labels.node }} Not Ready'
                  description: 'Node {{ $labels.node }} has been unready for more than 5 minutes'
                  runbook: 'Check node status and kubelet service'

              - alert: HighNodeCPU
                expr: instance:node_cpu_utilisation:rate5m * 100 > 85
                for: 10m
                labels:
                  severity: warning
                annotations:
                  title: '‚ö†Ô∏è High CPU Usage on {{ $labels.instance }}'
                  description: 'Node {{ $labels.instance }} CPU usage is above 85% for more than 10 minutes'
                  runbook: 'Check system processes and consider scaling'

              - alert: HighNodeMemory
                expr: instance:node_memory_utilisation:ratio * 100 > 85
                for: 10m
                labels:
                  severity: warning
                annotations:
                  title: '‚ö†Ô∏è High Memory Usage on {{ $labels.instance }}'
                  description: 'Node {{ $labels.instance }} memory usage is above 85% for more than 10 minutes'
                  runbook: 'Check memory-heavy processes and consider scaling'

              - alert: LowDiskSpace
                expr: node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"} * 100 < 10
                for: 5m
                labels:
                  severity: warning
                annotations:
                  title: 'üíæ Low Disk Space on {{ $labels.instance }}'
                  description: 'Node {{ $labels.instance }} has less than 10% free disk space'
                  runbook: 'Clean up disk space or expand storage'

          - name: KubernetesAlerts
            rules:
              - alert: KubePodCrashLooping
                expr: rate(kube_pod_container_status_restarts_total[15m]) * 60 * 5 > 0
                for: 15m
                labels:
                  severity: warning
                annotations:
                  title: 'üîÑ Pod Crash Looping: {{ $labels.namespace }}/{{ $labels.pod }}'
                  description: 'Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping'
                  runbook: 'Check pod logs and events'

              - alert: KubePodPending
                expr: kube_pod_status_phase{phase="Pending"} == 1
                for: 15m
                labels:
                  severity: warning
                annotations:
                  title: '‚è≥ Pod Stuck Pending: {{ $labels.namespace }}/{{ $labels.pod }}'
                  description: 'Pod {{ $labels.namespace }}/{{ $labels.pod }} has been pending for more than 15 minutes'
                  runbook: 'Check pod events and resource constraints'

          - name: ControlPlaneAlerts
            rules:
              - alert: KubeAPIDown
                expr: up{job="apiserver"} == 0
                for: 5m
                labels:
                  severity: critical
                annotations:
                  title: 'üö® Kubernetes API Unreachable'
                  description: 'Kubernetes API is unreachable'
                  runbook: 'Check control plane components and API server logs'

              - alert: EtcdHighCommitDuration
                expr: histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket[5m])) > 0.5
                for: 10m
                labels:
                  severity: warning
                annotations:
                  title: '‚ö° Etcd High Commit Duration'
                  description: 'Etcd 99th percentile commit duration is too high'
                  runbook: 'Check etcd performance and disk I/O'

          - name: ResourceAlerts
            rules:
              - alert: PersistentVolumeUsageHigh
                expr: kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes * 100 > 85
                for: 5m
                labels:
                  severity: warning
                annotations:
                  title: 'üìä High PVC Usage: {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }}'
                  description: 'PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is using more than 85% of its capacity'
                  runbook: 'Consider expanding PVC or cleaning up data' 